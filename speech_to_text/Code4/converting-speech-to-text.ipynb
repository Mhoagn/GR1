{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":4727500,"sourceType":"datasetVersion","datasetId":2735613}],"dockerImageVersionId":30301,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"**Defining the objective**\n","metadata":{}},{"cell_type":"markdown","source":"The aim of this project is to create a Speech-To-Text application using an ASR (Automatic Speech Recognition) system, Whisper. ","metadata":{}},{"cell_type":"markdown","source":"Whisper has been trained for 680,000 hours on huge amount of speech data collected from the internet. The diverse dataset allows Whisper to understand different accents, and filter background noise. The Whisper architecture is a simple end-to-end approach, implemented as an encoder-decoder Transformer. Input audio is split into 30-second chunks, converted into a log-Mel spectrogram, and then passed into an encoder. A decoder is trained to predict the corresponding text caption, intermixed with special tokens that direct the single model to perform tasks such as language identification, phrase-level timestamps, multilingual speech transcription, and to-English speech translation.\n\n You can learn more about Whisper from https://openai.com/blog/whisper/","metadata":{}},{"cell_type":"markdown","source":"**Installing Required Libraries**","metadata":{}},{"cell_type":"code","source":"!pip install --upgrade torch\n!pip install pytube\n!pip install git+https://github.com/openai/whisper.git\n!pip install git+https://github.com/librosa/librosa","metadata":{"execution":{"iopub.status.busy":"2022-12-30T11:51:57.624357Z","iopub.execute_input":"2022-12-30T11:51:57.625363Z","iopub.status.idle":"2022-12-30T11:56:04.507824Z","shell.execute_reply.started":"2022-12-30T11:51:57.62532Z","shell.execute_reply":"2022-12-30T11:56:04.506024Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Importing the necessary libraries\nimport torch\nimport whisper\nimport pytube\nimport librosa\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport IPython.display as ipd","metadata":{"execution":{"iopub.status.busy":"2022-12-30T11:56:04.51211Z","iopub.execute_input":"2022-12-30T11:56:04.512569Z","iopub.status.idle":"2022-12-30T11:56:06.703042Z","shell.execute_reply.started":"2022-12-30T11:56:04.512493Z","shell.execute_reply":"2022-12-30T11:56:06.701847Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Please note:** If you don't already have those libraries installed, trying to import them would not work as they have to be installed first. ","metadata":{}},{"cell_type":"markdown","source":"**Loading the Model**","metadata":{}},{"cell_type":"markdown","source":"There are five model sizes to choose from, four have English-only versions, offering speed and accuracy trade-offs. The model sizes are:\n- tiny: 39M Parameters, English-only model (tiny.en), Multilingual model (tiny), Required VRAM (1GB), Relative speed (32x)\n- base: 74M Parameters, English-only model (base.en), Multilingual model (base), Required VRAM (1GB), Relative speed (16x)\n- small: 244M Parameters, English-only model (small.en), Multilingual model (small), Required VRAM (2GB), Relative speed (6x)\n- medium: 769M Parameters, English-only model (medium.en), Multilingual model (medium), Required VRAM (5GB), Relative speed (2x)\n- tiny: 1550M Parameters, English-only model (N/A), Multilingual model (large), Required VRAM (10GB), Relative speed (1x)\nThe tiny model can be utilized best for light weight applications, the large model if accuracy is most important, and the base, small or medium models for everything in between. For this project, we would be using the medium model.","metadata":{}},{"cell_type":"code","source":"model_m = whisper.load_model('medium')","metadata":{"execution":{"iopub.status.busy":"2022-12-30T11:56:06.704467Z","iopub.execute_input":"2022-12-30T11:56:06.705264Z","iopub.status.idle":"2022-12-30T11:56:54.537417Z","shell.execute_reply.started":"2022-12-30T11:56:06.705219Z","shell.execute_reply":"2022-12-30T11:56:54.536397Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Loading the file**","metadata":{}},{"cell_type":"markdown","source":"We start by loading an audio file.","metadata":{}},{"cell_type":"code","source":"file_path = '/kaggle/input/voice-recording/Record (online-voice-recorder.com).mp3'","metadata":{"execution":{"iopub.status.busy":"2022-12-30T11:56:54.538944Z","iopub.execute_input":"2022-12-30T11:56:54.539262Z","iopub.status.idle":"2022-12-30T11:56:54.543005Z","shell.execute_reply.started":"2022-12-30T11:56:54.539232Z","shell.execute_reply":"2022-12-30T11:56:54.542302Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"I created a custom voice recording of myself, in file_path above to use for this project. Next we're going to load the audio file in file_path using the load_audio() function.","metadata":{}},{"cell_type":"code","source":"#Loading\naudio_13 = whisper.load_audio(file_path)\naudio_13","metadata":{"execution":{"iopub.status.busy":"2022-12-30T11:56:54.545264Z","iopub.execute_input":"2022-12-30T11:56:54.545727Z","iopub.status.idle":"2022-12-30T11:56:55.296124Z","shell.execute_reply.started":"2022-12-30T11:56:54.545697Z","shell.execute_reply":"2022-12-30T11:56:55.294705Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Next, we find the sampling interval. The sampling interval is the distance or time between the measurements. The total time of audio is 13 seconds.","metadata":{}},{"cell_type":"code","source":"T = 13","metadata":{"execution":{"iopub.status.busy":"2022-12-30T11:56:55.298048Z","iopub.execute_input":"2022-12-30T11:56:55.298546Z","iopub.status.idle":"2022-12-30T11:56:55.303871Z","shell.execute_reply.started":"2022-12-30T11:56:55.29846Z","shell.execute_reply":"2022-12-30T11:56:55.302575Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Checking the number of samples in our audio file\nn_samples =  audio_13.shape[0]\nn_samples","metadata":{"execution":{"iopub.status.busy":"2022-12-30T11:56:55.305083Z","iopub.execute_input":"2022-12-30T11:56:55.305359Z","iopub.status.idle":"2022-12-30T11:56:55.316501Z","shell.execute_reply.started":"2022-12-30T11:56:55.305333Z","shell.execute_reply":"2022-12-30T11:56:55.315594Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"There are 200448 number of samples in 13 seconds audio. Now we find the time between samples.","metadata":{}},{"cell_type":"code","source":"#Time between samples\ndelta = T/n_samples\ndelta","metadata":{"execution":{"iopub.status.busy":"2022-12-30T11:56:55.318195Z","iopub.execute_input":"2022-12-30T11:56:55.318735Z","iopub.status.idle":"2022-12-30T11:56:55.329286Z","shell.execute_reply.started":"2022-12-30T11:56:55.318697Z","shell.execute_reply":"2022-12-30T11:56:55.328422Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The time between samples is 6.485472541507024e-05. Next, we find the sampling frequency.","metadata":{}},{"cell_type":"code","source":"#Sampling frequency\nFs = 1/delta\nFs","metadata":{"execution":{"iopub.status.busy":"2022-12-30T11:56:55.331279Z","iopub.execute_input":"2022-12-30T11:56:55.331719Z","iopub.status.idle":"2022-12-30T11:56:55.340193Z","shell.execute_reply.started":"2022-12-30T11:56:55.331689Z","shell.execute_reply":"2022-12-30T11:56:55.339037Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The sampling frequency is 15419.076923076924. Next, we find the time of each sample.","metadata":{}},{"cell_type":"code","source":"#Time of each sample\ntime = np.linspace(0,(n_samples-1) * delta,n_samples)\ntime","metadata":{"execution":{"iopub.status.busy":"2022-12-30T11:56:55.341576Z","iopub.execute_input":"2022-12-30T11:56:55.341996Z","iopub.status.idle":"2022-12-30T11:56:55.352913Z","shell.execute_reply.started":"2022-12-30T11:56:55.341953Z","shell.execute_reply":"2022-12-30T11:56:55.351867Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Now we plot the amplitude with respect to time:","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(20,10))\nplt.title('Signal')\nplt.plot(time,audio_13)\nplt.ylabel('amplitude')\nplt.xlabel('seconds')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-12-30T11:56:55.354252Z","iopub.execute_input":"2022-12-30T11:56:55.354682Z","iopub.status.idle":"2022-12-30T11:56:55.764488Z","shell.execute_reply.started":"2022-12-30T11:56:55.354652Z","shell.execute_reply":"2022-12-30T11:56:55.763242Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Above is a waveform for the signal. Now, we can use the pad_or_trim() method to ensure the sample is in the right form for inference.","metadata":{}},{"cell_type":"code","source":"audio = whisper.pad_or_trim(audio_13)","metadata":{"execution":{"iopub.status.busy":"2022-12-30T11:56:55.765924Z","iopub.execute_input":"2022-12-30T11:56:55.766253Z","iopub.status.idle":"2022-12-30T11:56:55.772605Z","shell.execute_reply.started":"2022-12-30T11:56:55.766223Z","shell.execute_reply":"2022-12-30T11:56:55.77134Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Next, we plot the amplitude with respect to time with trimmed/padded audio.","metadata":{}},{"cell_type":"code","source":"#Number of samples in our trimmed/padded audio\nn_samples =  audio.shape[-1]\n#Time of each sample\ntime = np.linspace(0,(n_samples-1)*delta,n_samples)","metadata":{"execution":{"iopub.status.busy":"2022-12-30T11:56:55.774183Z","iopub.execute_input":"2022-12-30T11:56:55.774557Z","iopub.status.idle":"2022-12-30T11:56:55.784128Z","shell.execute_reply.started":"2022-12-30T11:56:55.774498Z","shell.execute_reply":"2022-12-30T11:56:55.783095Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize=(20,10))\nplt.title('Signal')\nplt.plot(time,audio)\nplt.ylabel('amplitude')\nplt.xlabel('seconds')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-12-30T11:56:55.787607Z","iopub.execute_input":"2022-12-30T11:56:55.787923Z","iopub.status.idle":"2022-12-30T11:56:56.159658Z","shell.execute_reply.started":"2022-12-30T11:56:55.787894Z","shell.execute_reply":"2022-12-30T11:56:56.158667Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Next, we can start plotting a mel spectogram by applying a log_mel_spectogram() funtion to our audio file. It converts the y-axis (frequency) into the mel scale:","metadata":{}},{"cell_type":"code","source":"mel = whisper.log_mel_spectrogram(audio).to(model_m.device)","metadata":{"execution":{"iopub.status.busy":"2022-12-30T11:56:56.161007Z","iopub.execute_input":"2022-12-30T11:56:56.161708Z","iopub.status.idle":"2022-12-30T11:56:56.178654Z","shell.execute_reply.started":"2022-12-30T11:56:56.161675Z","shell.execute_reply":"2022-12-30T11:56:56.17782Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The output above is a tensor of converted frequencies. Now, we plot 2 subplots, one is a regular representation of sound amplitude over period of time, and the other is our mel spectrogram:","metadata":{}},{"cell_type":"code","source":"fig, (ax1, ax2) = plt.subplots(2)\nfig.tight_layout(pad=5.0)\nax1.plot(time,audio)\nax1.set_title('Signal')\nax1.set_xlabel('Time, seconds')\nax1.set_ylabel('Amplitude')\nax2.imshow((mel.numpy()*mel.numpy())**(1/2),interpolation='nearest', aspect='auto')\nax2.set_title('Mel Spectrogram of a Signal')\nax2.set_xlabel('Time, seconds')\nax2.set_ylabel('Mel Scale')","metadata":{"execution":{"iopub.status.busy":"2022-12-30T11:56:56.180032Z","iopub.execute_input":"2022-12-30T11:56:56.180325Z","iopub.status.idle":"2022-12-30T11:56:56.538537Z","shell.execute_reply.started":"2022-12-30T11:56:56.180297Z","shell.execute_reply":"2022-12-30T11:56:56.537703Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Next, we can move on to language detection.","metadata":{}},{"cell_type":"markdown","source":"**Language detection**","metadata":{}},{"cell_type":"markdown","source":"We will listen to our audio file and detect the spoken language. The sample rate (sr) by default is 22050, which means that for every second there are 22,050 samples. We can use ipd.Audio() function to listen to our audio file.","metadata":{}},{"cell_type":"code","source":"sr=22050\nipd.Audio(audio, rate=sr)","metadata":{"execution":{"iopub.status.busy":"2022-12-30T11:56:56.539775Z","iopub.execute_input":"2022-12-30T11:56:56.540237Z","iopub.status.idle":"2022-12-30T11:56:56.576648Z","shell.execute_reply.started":"2022-12-30T11:56:56.540208Z","shell.execute_reply":"2022-12-30T11:56:56.573348Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Next, We can obntain the probability of each language by using detect_language() method.","metadata":{}},{"cell_type":"code","source":"probs = model_m.detect_language(mel)","metadata":{"execution":{"iopub.status.busy":"2022-12-30T12:01:52.738322Z","iopub.execute_input":"2022-12-30T12:01:52.738829Z","iopub.status.idle":"2022-12-30T12:02:12.689563Z","shell.execute_reply.started":"2022-12-30T12:01:52.73878Z","shell.execute_reply":"2022-12-30T12:02:12.688574Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"probs","metadata":{"execution":{"iopub.status.busy":"2022-12-30T12:02:59.667833Z","iopub.execute_input":"2022-12-30T12:02:59.668273Z","iopub.status.idle":"2022-12-30T12:02:59.679224Z","shell.execute_reply.started":"2022-12-30T12:02:59.668239Z","shell.execute_reply":"2022-12-30T12:02:59.678033Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"From above, we can see the probability of each language being the spoken language. English has the highest probability of 98.5%, therefore it is the spoken language in the audio file.","metadata":{}},{"cell_type":"markdown","source":"Next, we can move on to transcription.","metadata":{}},{"cell_type":"markdown","source":"**Transcription**","metadata":{}},{"cell_type":"code","source":"transcription = model_m.transcribe(file_path, fp16 = False)['text']","metadata":{"execution":{"iopub.status.busy":"2022-12-30T12:20:14.952571Z","iopub.execute_input":"2022-12-30T12:20:14.95313Z","iopub.status.idle":"2022-12-30T12:21:07.713235Z","shell.execute_reply.started":"2022-12-30T12:20:14.953073Z","shell.execute_reply":"2022-12-30T12:21:07.711561Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"transcription","metadata":{"execution":{"iopub.status.busy":"2022-12-30T12:22:00.804254Z","iopub.execute_input":"2022-12-30T12:22:00.805188Z","iopub.status.idle":"2022-12-30T12:22:00.813866Z","shell.execute_reply.started":"2022-12-30T12:22:00.805127Z","shell.execute_reply":"2022-12-30T12:22:00.81258Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"From the transcription above, we can see our Speech-To-Text system works very well, it transcribed our audio perfectly.","metadata":{}},{"cell_type":"markdown","source":"As an adition, we can translate our audio file to another language.","metadata":{}},{"cell_type":"markdown","source":"**Translation**","metadata":{}},{"cell_type":"markdown","source":"We would translate our audio file to Spanish, by setting language='es'.","metadata":{}},{"cell_type":"code","source":"translation = model_m.transcribe(file_path, language = 'es', fp16 = False)['text']","metadata":{"execution":{"iopub.status.busy":"2022-12-30T12:34:22.005581Z","iopub.execute_input":"2022-12-30T12:34:22.006077Z","iopub.status.idle":"2022-12-30T12:35:04.182227Z","shell.execute_reply.started":"2022-12-30T12:34:22.00604Z","shell.execute_reply":"2022-12-30T12:35:04.180344Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"translation","metadata":{"execution":{"iopub.status.busy":"2022-12-30T12:35:26.090791Z","iopub.execute_input":"2022-12-30T12:35:26.091308Z","iopub.status.idle":"2022-12-30T12:35:26.100432Z","shell.execute_reply.started":"2022-12-30T12:35:26.091254Z","shell.execute_reply":"2022-12-30T12:35:26.098943Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Our audio file can be translated to other languages as well.","metadata":{}},{"cell_type":"markdown","source":"**Conclusion**","metadata":{}},{"cell_type":"markdown","source":"We have been able to create a Speech-To-Text application using an ASR (Automatic Speech Recognition) system, Whisper. Speech-To-Text has a variety of use cases which include transcribing audio recordings, dictation, voice commands, online search, enhanced customer service etc.","metadata":{}},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null}]}