{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":492337,"sourceType":"datasetVersion","datasetId":230420},{"sourceId":2739456,"sourceType":"datasetVersion","datasetId":1670098},{"sourceId":8182822,"sourceType":"datasetVersion","datasetId":4844866}],"dockerImageVersionId":30698,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torchaudio\nroot_path = '/kaggle/input/librispeech-clean'","metadata":{"execution":{"iopub.status.busy":"2024-04-27T16:24:42.047461Z","iopub.execute_input":"2024-04-27T16:24:42.047824Z","iopub.status.idle":"2024-04-27T16:24:46.404237Z","shell.execute_reply.started":"2024-04-27T16:24:42.047793Z","shell.execute_reply":"2024-04-27T16:24:46.403451Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_dataset = torchaudio.datasets.LIBRISPEECH(root_path, url=\"train-clean-100\", download=False)\ntest_dataset = torchaudio.datasets.LIBRISPEECH(root_path, url=\"test-clean\", download=False)","metadata":{"execution":{"iopub.status.busy":"2024-04-27T16:25:43.94489Z","iopub.execute_input":"2024-04-27T16:25:43.945413Z","iopub.status.idle":"2024-04-27T16:26:02.340177Z","shell.execute_reply.started":"2024-04-27T16:25:43.945382Z","shell.execute_reply":"2024-04-27T16:26:02.339238Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Visulalize wavefrom\nimport matplotlib.pyplot as plt\nwaveform, sample_rate, utterance, speaker_id, chapter_id, utterance_id = train_dataset[0]\n# plot the waveform\nplt.figure(figsize =(12, 4))\nplt.plot(waveform.t().numpy())\nplt.title(f\"Waaveform of audio smaple\\n{utterance}\\n speaker id: {speaker_id}, chapter id: {chapter_id}, utterance id: {utterance_id}\")\nplt.xlabel('Time')\nplt.ylabel('Amplitude')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-04-27T16:26:05.655829Z","iopub.execute_input":"2024-04-27T16:26:05.65652Z","iopub.status.idle":"2024-04-27T16:26:06.233648Z","shell.execute_reply.started":"2024-04-27T16:26:05.656487Z","shell.execute_reply":"2024-04-27T16:26:06.23266Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import IPython.display as ipd\nipd.Audio(waveform.numpy(), rate=sample_rate)","metadata":{"execution":{"iopub.status.busy":"2024-04-27T16:26:08.685473Z","iopub.execute_input":"2024-04-27T16:26:08.685879Z","iopub.status.idle":"2024-04-27T16:26:08.708169Z","shell.execute_reply.started":"2024-04-27T16:26:08.685846Z","shell.execute_reply":"2024-04-27T16:26:08.70725Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Ars Metrics","metadata":{}},{"cell_type":"code","source":"import numpy as np\ndef levenshtein_distance(ref, hyp):\n    \"\"\"Calculate the Levenshtien distance between two sequences\"\"\"\n    m, n = len(ref), len(hyp)\n    if ref == hyp:\n        return 0\n    if m == 0:\n        return n\n    if n == 0:\n        return m\n    \n    if m < n :\n        ref, hyp = hyp , ref\n        m , n = n, m\n        \n    previous_row = range(n + 1)\n    for i, c1 in enumerate(ref):\n        current_row = [ i + 1]\n        for j , c2 in enumerate(hyp):\n            insertions = previous_row[ j + 1] + 1\n            deletions = current_row[j] + 1\n            substitutions = previous_row[j] + (c1 != c2)\n            current_row.append(min(insertions, deletions, substitutions))\n            \n        previous_row = current_row\n    return previous_row[-1]\n\ndef normalize_text(text, ignore_case = False, remove_space = False):\n    \"\"\"Normalize text based on the specified conditions.\"\"\"\n    if ignore_case:\n        text = text.lower()\n    if remove_space:\n        text = ''.join(text.split())\n    return text\n\ndef calculate_errors(reference, hypothesis, ignore_case=False, remove_space=False, delimiter=None):\n    \"\"\"Calculates edit distance and length/reference length based on the mode (word or char level).\"\"\"\n    reference = normalize_text(reference, ignore_case, remove_space)\n    hypothesis = normalize_text(hypothesis, ignore_case, remove_space)\n\n    if delimiter:\n        reference = reference.split(delimiter)\n        hypothesis = hypothesis.split(delimiter)\n\n    edit_distance = levenshtein_distance(reference, hypothesis)\n    ref_len = len(reference)\n\n    return float(edit_distance), ref_len\n\ndef wer(reference, hypothesis, ignore_case=False, delimiter=' '):\n    \"\"\"Calculates word error rate.\"\"\"\n    edit_distance, ref_len = calculate_errors(reference, hypothesis, ignore_case, False, delimiter)\n\n    if ref_len == 0:\n        raise ValueError(\"Reference's word number should be greater than 0.\")\n\n    return edit_distance / ref_len\n\ndef cer(reference, hypothesis, ignore_case=False, remove_space=False):\n    \"\"\"Calculates character error rate.\"\"\"\n    edit_distance, ref_len = calculate_errors(reference, hypothesis, ignore_case, remove_space)\n\n    if ref_len == 0:\n        raise ValueError(\"Length of reference should be greater than 0.\")\n\n    return edit_distance / ref_len\n            ","metadata":{"execution":{"iopub.status.busy":"2024-04-27T16:26:38.048279Z","iopub.execute_input":"2024-04-27T16:26:38.049274Z","iopub.status.idle":"2024-04-27T16:26:38.06288Z","shell.execute_reply.started":"2024-04-27T16:26:38.049232Z","shell.execute_reply":"2024-04-27T16:26:38.061981Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nprint(os.environ.get('CUDA_VISIBLE_DEVICES'))","metadata":{"execution":{"iopub.status.busy":"2024-04-27T16:27:27.444949Z","iopub.execute_input":"2024-04-27T16:27:27.445355Z","iopub.status.idle":"2024-04-27T16:27:27.451001Z","shell.execute_reply.started":"2024-04-27T16:27:27.445322Z","shell.execute_reply":"2024-04-27T16:27:27.449963Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"cer('azizul', 'aziz')","metadata":{"execution":{"iopub.status.busy":"2024-04-27T16:27:30.965139Z","iopub.execute_input":"2024-04-27T16:27:30.965546Z","iopub.status.idle":"2024-04-27T16:27:30.971506Z","shell.execute_reply.started":"2024-04-27T16:27:30.965516Z","shell.execute_reply":"2024-04-27T16:27:30.970502Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"reference = 'Hello My Name is Hakim'\nhypothesis = 'Hello My Neim is Hakim'\n\n# Calculate cer\ncer_result = cer(reference, hypothesis)\nprint(f\"CER: {cer_result}\")\n\n# calculate WER\nwer_result = wer(reference, hypothesis)\nprint(f\"WER: {wer_result}\")","metadata":{"execution":{"iopub.status.busy":"2024-04-27T16:27:31.94503Z","iopub.execute_input":"2024-04-27T16:27:31.945673Z","iopub.status.idle":"2024-04-27T16:27:31.95168Z","shell.execute_reply.started":"2024-04-27T16:27:31.945643Z","shell.execute_reply":"2024-04-27T16:27:31.95063Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Data Preparation\n\nText Transformation","metadata":{}},{"cell_type":"code","source":"import string\nfrom itertools import dropwhile\n\nclass TextTransform:\n    \"\"\"Maps characters to integers and vice verse, using the string module for cahracter definitions\"\"\"\n    \n    def __init__(self):\n        # Including additional characters manually and using string.ascii_lowercase for latters\n        additional_chars = [\"''\", '<SPACE>']\n        all_chars = additional_chars + list(string.ascii_lowercase)\n        \n        # Generatring char_map with enumeration, starting indices from 0\n        self.char_map = {char: i for i, char in enumerate(all_chars)}\n        # Inverting char_map to crate index_map\n        self.index_map = {index: char for char, index in self.char_map.items()}\n        self.index_map[self.char_map['<SPACE>']] = ' ' \n        \n    def text_to_int(self, text):\n        '''Converts text to an interger sequences using a character map'''\n        return [self.char_map.get(c, self.char_map['<SPACE>']) for c in text.lower()]\n    \n    def int_to_text(self, labels):\n        '''converts integer labels to a text sequence using a character map'''\n        return ''.join(self.index_map[i] for i in labels).replace('<SPACE>', ' ')\n    \n    def int_to_text_remove_pad(self, int_sequence):\n        # Remove trailing zeros which are used for padding\n        # This is done by reversing the list, using itertools.dropwhile to drop the zeros,\n        # and then reversing the list back to the original order\n        from itertools import dropwhile\n\n        # Reverse, remove zeros from the end (now start), and reverse back\n        text_sequence = list(dropwhile(lambda x: x == 0, reversed(int_sequence)))[::-1]\n\n        # Convert the non-padded sequence to text\n        text = ''.join(self.index_map[i] for i in text_sequence).replace('<SPACE>', ' ')\n        return text\n    \n    \n# Example Usage \ntext_transform = TextTransform()\ntest_text = 'This is Hakim'\ntest_ints = text_transform.text_to_int(test_text)\nrecovered_text = text_transform.int_to_text(test_ints)\n\nprint(f\"original: {test_text}\")\nprint(f\"Converted to Int : {test_ints}\")\nprint(f\"Recoverd text: {recovered_text}\")","metadata":{"execution":{"iopub.status.busy":"2024-04-27T16:27:33.856632Z","iopub.execute_input":"2024-04-27T16:27:33.857283Z","iopub.status.idle":"2024-04-27T16:27:33.869204Z","shell.execute_reply.started":"2024-04-27T16:27:33.857251Z","shell.execute_reply":"2024-04-27T16:27:33.868121Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_int = [21, 9, 10, 20, 1, 10, 20, 1, 9, 2, 12, 10, 14]\nprint('star: ' + text_transform.int_to_text_remove_pad(test_int) + \"|end\")","metadata":{"execution":{"iopub.status.busy":"2024-04-27T16:27:35.565853Z","iopub.execute_input":"2024-04-27T16:27:35.566544Z","iopub.status.idle":"2024-04-27T16:27:35.571902Z","shell.execute_reply.started":"2024-04-27T16:27:35.566511Z","shell.execute_reply":"2024-04-27T16:27:35.570894Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# How to pad batches","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\ntext_transform = TextTransform()\n\ntexts = [\"Hello world\", \"My name is Hakim\"]\nlabels = [torch.Tensor(text_transform.text_to_int(text)) for text in texts]\nlabels = nn.utils.rnn.pad_sequence(labels, batch_first=True)\nlabels","metadata":{"execution":{"iopub.status.busy":"2024-04-27T16:27:39.256084Z","iopub.execute_input":"2024-04-27T16:27:39.256457Z","iopub.status.idle":"2024-04-27T16:27:39.312279Z","shell.execute_reply.started":"2024-04-27T16:27:39.256427Z","shell.execute_reply":"2024-04-27T16:27:39.311307Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Convert Audio data","metadata":{}},{"cell_type":"code","source":"train_audio_transforms = nn.Sequential(\n    torchaudio.transforms.MelSpectrogram(sample_rate = 16000, n_mels = 128),\n    torchaudio.transforms.FrequencyMasking(freq_mask_param=30),\n    torchaudio.transforms.TimeMasking(time_mask_param=100)\n)\n\nvalid_audio_transforms = torchaudio.transforms.MelSpectrogram()\n\nwaveform, sample_rate, utterance, speaker_id, chapter_id, utterance_id = train_dataset[0]\nspec = train_audio_transforms(waveform).squeeze(0).transpose(0, 1)\nspec.shape","metadata":{"execution":{"iopub.status.busy":"2024-04-27T16:27:41.074617Z","iopub.execute_input":"2024-04-27T16:27:41.075667Z","iopub.status.idle":"2024-04-27T16:27:41.182076Z","shell.execute_reply.started":"2024-04-27T16:27:41.075626Z","shell.execute_reply":"2024-04-27T16:27:41.181101Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# convert specs of different sizes to batches","metadata":{}},{"cell_type":"code","source":"spectrograms = []\nfor waveform, sample_rate, utterance, speaker_id, chapter_id, utterance_id in [train_dataset[0], train_dataset[1]]:\n    spec = train_audio_transforms(waveform).squeeze(0).transpose(0, 1)\n    spectrograms.append(spec)\nspectrograms = nn.utils.rnn.pad_sequence(spectrograms, batch_first=True).unsqueeze(1).transpose(2, 3)\nspectrograms.size()","metadata":{"execution":{"iopub.status.busy":"2024-04-27T16:27:43.675085Z","iopub.execute_input":"2024-04-27T16:27:43.675934Z","iopub.status.idle":"2024-04-27T16:27:43.716054Z","shell.execute_reply.started":"2024-04-27T16:27:43.6759Z","shell.execute_reply":"2024-04-27T16:27:43.715143Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Full Data Loading","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\ndef data_processing(data, data_type = 'train'):\n    if data_type not in ['train', 'valid']:\n        raise ValueError(\"data_type should be either 'trian' or 'valid.'\")\n        \n    spectrograms = []\n    labels = []\n    input_lengths = []\n    label_lengths = []\n    \n    audio_transforms = train_audio_transforms if data_type == 'train' else valid_audio_transforms\n    \n    for (waveform, _, utterance, _, _, _) in data:\n        spec = audio_transforms(waveform).squeeze(0).transpose(0, 1)\n        spectrograms.append(spec)\n        label = torch.Tensor(text_transform.text_to_int(utterance.lower()))\n        labels.append(label)\n        input_lengths.append(spec.shape[0] // 2)\n        label_lengths.append(len(label))\n        \n    spectrograms = nn.utils.rnn.pad_sequence(spectrograms, batch_first = True).unsqueeze(1).transpose(2,3)\n    labels = nn.utils.rnn.pad_sequence(labels, batch_first = True)\n    return spectrograms, labels, input_lengths, label_lengths","metadata":{"execution":{"iopub.status.busy":"2024-04-27T16:27:45.845835Z","iopub.execute_input":"2024-04-27T16:27:45.84622Z","iopub.status.idle":"2024-04-27T16:27:45.855318Z","shell.execute_reply.started":"2024-04-27T16:27:45.846171Z","shell.execute_reply":"2024-04-27T16:27:45.854246Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch.utils.data as data\n\npipeline_params = {\n    'batch_size': 10,\n    'epochs': 1,\n    'learning_rate': 5e-4,\n    'n_cnn_layers': 3, \n    'n_rnn_layers':5,\n    'rnn_dim': 512,\n    'n_class': 29,\n    'n_feats':  128,\n    'stride': 2,\n    'dropout': 0.1\n}\n\nkwargs = {'num_workers':1, 'pin_memory': True}\ntrain_loader = data.DataLoader(dataset = train_dataset,\n                              batch_size = pipeline_params['batch_size'],\n                              shuffle = True,\n                              collate_fn = lambda x: data_processing(x, 'train'),\n                              **kwargs)\ntest_loader = data.DataLoader(dataset = test_dataset,\n                             batch_size = pipeline_params['batch_size'],\n                             shuffle = True,\n                             collate_fn=lambda x: data_processing(x, 'valid'),\n                             **kwargs)","metadata":{"execution":{"iopub.status.busy":"2024-04-27T16:28:06.947516Z","iopub.execute_input":"2024-04-27T16:28:06.948222Z","iopub.status.idle":"2024-04-27T16:28:06.955773Z","shell.execute_reply.started":"2024-04-27T16:28:06.948167Z","shell.execute_reply":"2024-04-27T16:28:06.954643Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"use_cuda = torch.cuda.is_available()\nprint(use_cuda)\ndevice = torch.device('cuda' if use_cuda else 'cpu')\n\nprint('Picked Device', device)","metadata":{"execution":{"iopub.status.busy":"2024-04-27T16:27:50.353004Z","iopub.execute_input":"2024-04-27T16:27:50.353402Z","iopub.status.idle":"2024-04-27T16:27:50.380203Z","shell.execute_reply.started":"2024-04-27T16:27:50.353371Z","shell.execute_reply":"2024-04-27T16:27:50.378971Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Modeling","metadata":{}},{"cell_type":"code","source":"use_cuda = torch.cuda.is_available()\nprint(use_cuda)\ndevice = torch.device(\"cuda\" if use_cuda else \"cpu\")\n\nprint(\"Picked device:\", device)\n\n\nclass CNNLayerNorm(nn.Module):\n    \"\"\"Layer normalization built for cnns input\"\"\"\n    def __init__(self, n_feats):\n        super(CNNLayerNorm, self).__init__()\n        self.layer_norm = nn.LayerNorm(n_feats)\n\n    def forward(self, x):\n        # x (batch, channel, feature, time)\n        x = x.transpose(2, 3).contiguous() # (batch, channel, time, feature)\n        x = self.layer_norm(x)\n        return x.transpose(2, 3).contiguous() # (batch, channel, feature, time)\n\nclass ResidualCNN(nn.Module):\n    \"\"\"\n    Residual CNN inspired by https://arxiv.org/pdf/1603.05027.pdf except with layer norm instead of batch norm\n    \"\"\"\n    def __init__(self, in_channels, out_channels, kernel, stride, dropout, n_feats):\n        super(ResidualCNN, self).__init__()\n\n        self.cnn1 = nn.Conv2d(in_channels, out_channels, kernel, stride, padding=kernel//2)\n        self.cnn2 = nn.Conv2d(out_channels, out_channels, kernel, stride, padding=kernel//2)\n        self.dropout1 = nn.Dropout(dropout)\n        self.dropout2 = nn.Dropout(dropout)\n        self.layer_norm1 = CNNLayerNorm(n_feats)\n        self.layer_norm2 = CNNLayerNorm(n_feats)\n\n    def forward(self, x):\n        residual = x  # (batch, channel, feature, time)\n        x = self.layer_norm1(x)\n        x = F.gelu(x)\n        x = self.dropout1(x)\n        x = self.cnn1(x)\n        x = self.layer_norm2(x)\n        x = F.gelu(x)\n        x = self.dropout2(x)\n        x = self.cnn2(x)\n        x += residual\n        return x # (batch, channel, feature, time)\n\nclass BidirectionalGRU(nn.Module):\n\n    def __init__(self, rnn_dim, hidden_size, dropout, batch_first):\n        super(BidirectionalGRU, self).__init__()\n\n        self.BiGRU = nn.GRU(\n            input_size=rnn_dim, hidden_size=hidden_size,\n            num_layers=1, batch_first=batch_first, bidirectional=True)\n        self.layer_norm = nn.LayerNorm(rnn_dim)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        x = self.layer_norm(x)\n        x = F.gelu(x)\n        x, _ = self.BiGRU(x)\n        x = self.dropout(x)\n        return x","metadata":{"execution":{"iopub.status.busy":"2024-04-27T16:28:14.454956Z","iopub.execute_input":"2024-04-27T16:28:14.455699Z","iopub.status.idle":"2024-04-27T16:28:14.470876Z","shell.execute_reply.started":"2024-04-27T16:28:14.455666Z","shell.execute_reply":"2024-04-27T16:28:14.469937Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Speech Recognition Model","metadata":{}},{"cell_type":"code","source":"class SpeechRecognitionModel(nn.Module):\n    \"\"\"Speech Recognition Model Inspired by DeepSpeech 2\"\"\"\n\n    def __init__(self, n_cnn_layers, n_rnn_layers, rnn_dim, n_class, n_feats, stride=2, dropout=0.1):\n        super(SpeechRecognitionModel, self).__init__()\n        n_feats = n_feats//2\n        self.cnn = nn.Conv2d(1, 32, 3, stride=stride, padding=3//2)  # cnn for extracting heirachal features\n\n        # n residual cnn layers with filter size of 32\n        self.rescnn_layers = nn.Sequential(*[\n            ResidualCNN(32, 32, kernel=3, stride=1, dropout=dropout, n_feats=n_feats)\n            for _ in range(n_cnn_layers)\n        ])\n        self.fully_connected = nn.Linear(n_feats*32, rnn_dim)\n        self.birnn_layers = nn.Sequential(*[\n            BidirectionalGRU(rnn_dim=rnn_dim if i==0 else rnn_dim*2,\n                             hidden_size=rnn_dim, dropout=dropout, batch_first=i==0)\n            for i in range(n_rnn_layers)\n        ])\n        self.classifier = nn.Sequential(\n            nn.Linear(rnn_dim*2, rnn_dim),  # birnn returns rnn_dim*2\n            nn.GELU(),\n            nn.Dropout(dropout),\n            nn.Linear(rnn_dim, n_class)\n        )\n\n    def forward(self, x):\n        x = self.cnn(x)\n        x = self.rescnn_layers(x)\n        sizes = x.size()\n        x = x.view(sizes[0], sizes[1] * sizes[2], sizes[3])  # (batch, feature, time)\n        x = x.transpose(1, 2) # (batch, time, feature)\n        x = self.fully_connected(x)\n        x = self.birnn_layers(x)\n        x = self.classifier(x)\n        return x\n\nmodel = SpeechRecognitionModel(pipeline_params['n_cnn_layers'], pipeline_params['n_rnn_layers'], pipeline_params['rnn_dim'], pipeline_params['n_class'], pipeline_params['n_feats'], pipeline_params['stride'], pipeline_params['dropout']).to(device)","metadata":{"execution":{"iopub.status.busy":"2024-04-27T16:28:49.945681Z","iopub.execute_input":"2024-04-27T16:28:49.946387Z","iopub.status.idle":"2024-04-27T16:28:50.282443Z","shell.execute_reply.started":"2024-04-27T16:28:49.946352Z","shell.execute_reply":"2024-04-27T16:28:50.28145Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom tqdm import tqdm\n\noptimizer = optim.AdamW(model.parameters(), pipeline_params[\"learning_rate\"])\n\ncriterion = nn.CTCLoss(blank=28).to(device)\n\nscheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr=pipeline_params[\"learning_rate\"],\n                                        steps_per_epoch=int(len(train_loader)),\n                                        epochs=pipeline_params[\"epochs\"],\n                                        anneal_strategy=\"linear\")\n\ndata_len = len(train_loader.dataset)\nlogging_idx = 0\nlogging_freq = 100\nfor epoch in range(pipeline_params[\"epochs\"]):\n    progress_bar = tqdm(enumerate(train_loader), total=len(train_loader), desc=f\"Epoch {epoch+1}/{pipeline_params['epochs']}\", unit=\"batches\")\n    for batch_idx, _data in progress_bar:\n        spectrograms, labels, input_lengths, label_lengths = _data\n        spectrograms, labels = spectrograms.to(device), labels.to(device)\n\n        optimizer.zero_grad()\n\n        output = model(spectrograms)\n        output = F.log_softmax(output, dim=2)\n        output = output.transpose(0, 1)\n\n        loss = criterion(output, labels, input_lengths, label_lengths)\n        loss.backward()\n\n        optimizer.step()\n        scheduler.step()\n        if logging_idx % logging_freq == 0:\n            print(\"Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n                epoch, \n                batch_idx * len(spectrograms), \n                data_len, 100. * batch_idx / len(train_loader), \n                loss.item()\n            ))\n        logging_idx += 1\n        progress_bar.set_postfix({'loss': loss.item()})\n","metadata":{"execution":{"iopub.status.busy":"2024-04-27T16:28:50.845703Z","iopub.execute_input":"2024-04-27T16:28:50.846449Z","iopub.status.idle":"2024-04-27T16:42:45.065121Z","shell.execute_reply.started":"2024-04-27T16:28:50.846419Z","shell.execute_reply":"2024-04-27T16:42:45.063964Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Saving the model\n# Save the model checkpoint\nmodel_path = '/kaggle/working/speech_recognition_model.pth'\ntorch.save(model.state_dict(), model_path)","metadata":{"execution":{"iopub.status.busy":"2024-04-27T16:43:00.388337Z","iopub.execute_input":"2024-04-27T16:43:00.388701Z","iopub.status.idle":"2024-04-27T16:43:00.567242Z","shell.execute_reply.started":"2024-04-27T16:43:00.388668Z","shell.execute_reply":"2024-04-27T16:43:00.566249Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Decoding","metadata":{}},{"cell_type":"code","source":"text_transform = TextTransform()\n\n# Updated greedy_decode to work with textTransform\ndef greedy_decode(outputs, text_trasform):\n    _, max_indices = torch.max(outputs, dim =2)\n    print('max_indices')\n    print(max_indices)\n    transcriptions = []\n    for indices in tqdm(max_indices):\n        transcription = ''\n        for index in indices:\n            if index.item() == text_transform.char_map['<SPACE>']:\n                continue\n                \n            character = text_transform.index_map[index.item()]\n            transcriptions += character\n        transcriptions.append(transcription)\n        \n    return transcriptions","metadata":{"execution":{"iopub.status.busy":"2024-04-27T16:43:07.980694Z","iopub.execute_input":"2024-04-27T16:43:07.981093Z","iopub.status.idle":"2024-04-27T16:43:07.988089Z","shell.execute_reply.started":"2024-04-27T16:43:07.981053Z","shell.execute_reply":"2024-04-27T16:43:07.987106Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Validation Test","metadata":{}},{"cell_type":"code","source":"import torch\n\ndef greedy_decoder(output_probs, blank_label=28):\n    \"\"\"Decodes the output probabilities to the most likely indices sequence.\"\"\"\n    max_probs, indices = torch.max(output_probs, dim=2)\n    decoded_batches = []\n\n    for idx, sequence in enumerate(indices.transpose(0, 1)):\n        previous = -1\n        decoded_sequence = []\n\n        for label_index in sequence:\n            if label_index != previous and label_index != blank_label:\n                decoded_sequence.append(label_index.item())\n            previous = label_index\n\n        decoded_batches.append(decoded_sequence)\n\n    return decoded_batches\n\ndef pad_sequences(sequences, pad_value=0):\n    \"\"\"Pads the sequences to the same length based on the longest sequence and returns a tensor.\"\"\"\n    max_length = max(len(seq) for seq in sequences)\n    padded_sequences = []\n\n    for seq in sequences:\n        padded_seq = seq + [pad_value] * (max_length - len(seq))\n        padded_sequences.append(padded_seq)\n\n    # Convert list of lists into a tensor\n    return torch.tensor(padded_sequences, dtype=torch.long)\n\n# Usage example with dummy data\nlogits = torch.randn(5, 2, 29)  # 5 time steps, 2 sequences, 29 classes\nprobs = torch.nn.functional.softmax(logits, dim=2)  # Convert logits to probabilities\n\ndecoded_sequences = greedy_decoder(probs)\npadded_decoded_sequences_tensor = pad_sequences(decoded_sequences)\n\nprint(\"Decoded and padded sequences as tensor:\", padded_decoded_sequences_tensor)","metadata":{"execution":{"iopub.status.busy":"2024-04-27T16:43:15.444619Z","iopub.execute_input":"2024-04-27T16:43:15.445254Z","iopub.status.idle":"2024-04-27T16:43:15.458286Z","shell.execute_reply.started":"2024-04-27T16:43:15.445219Z","shell.execute_reply":"2024-04-27T16:43:15.457345Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn.functional as F\ndef validate(model, validation_loader, criterion, device):\n    model.eval()\n    with torch.no_grad():\n        total_loss = 0\n        all_predicted_texts = []\n        all_true_texts = []\n        for batch_idx, _data in tqdm(enumerate(validation_loader)):\n            spectrograms, labels, input_lengths, label_lengths = _data\n            spectrograms, labels = spectrograms.to(device), labels.to(device)\n            \n            output = model(spectrograms)\n            output =F.log_softmax(output, dim = 2)\n            output = output.transpose(0, 1) # Needed for CTCLoss\n            loss = criterion(output, labels, input_lengths, label_lengths)\n            total_loss += loss.item()\n            decoded_outputs = greedy_decoder(output)\n            predicted_texts = [text_transform.int_to_text(seq) for seq in decoded_outputs]\n            true_texts  = [text_transform.int_to_text(label.tolist()) for label in labels]\n            \n            all_predicted_texts.extend(predicted_texts)\n            all_true_texts.extend(true_texts)\n            \n        avg_loss = total_loss / len(validation_loader)\n        avg_wer = np.mean([wer(ref, hyp) for ref, hyp in zip(all_true_texts,all_predicted_texts)])\n        avg_cer = np.mean([cer(ref, hyp) for ref, hyp in zip(all_true_texts, all_predicted_texts)])\n        \n        print(f\"Validation Loss: {avg_loss}\")\n        print(f\"Average WER: {avg_wer:.4f}\")\n        print(f\"Average CER: {avg_cer:.4f}\")\n        \n    ","metadata":{"execution":{"iopub.status.busy":"2024-04-27T16:43:17.612899Z","iopub.execute_input":"2024-04-27T16:43:17.613294Z","iopub.status.idle":"2024-04-27T16:43:17.62376Z","shell.execute_reply.started":"2024-04-27T16:43:17.613262Z","shell.execute_reply":"2024-04-27T16:43:17.622696Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"validate(model, test_loader, criterion, device)","metadata":{"execution":{"iopub.status.busy":"2024-04-27T16:43:18.555841Z","iopub.execute_input":"2024-04-27T16:43:18.556509Z","iopub.status.idle":"2024-04-27T16:45:44.460686Z","shell.execute_reply.started":"2024-04-27T16:43:18.556477Z","shell.execute_reply":"2024-04-27T16:45:44.459635Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# Function to load the saved model\ndef load_model(model_path):\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    loaded_model = torch.load(model_path, map_location=device)\n    return loaded_model\n\n# Function to perform speech-to-text conversion\ndef speech_to_text(audio_path, model, device, text_transform, valid_audio_transforms):\n    # Load the audio file\n    waveform, sample_rate = torchaudio.load(audio_path)\n    \n    # Apply audio transformations\n    waveform = valid_audio_transforms(waveform)\n    \n    # Create a tensor and move it to the device\n    waveform = waveform.unsqueeze(0).to(device)\n    \n    # Run the model on the audio input\n    output = model(waveform)\n    output = F.log_softmax(output, dim=2)\n    output = output.transpose(0, 1)\n    \n    # Decode the output probabilities to text\n    decoded_output = greedy_decoder(output)\n    predicted_text = text_transform.int_to_text(decoded_output[0])\n    \n    return predicted_text","metadata":{"execution":{"iopub.status.busy":"2024-04-27T16:46:00.559098Z","iopub.execute_input":"2024-04-27T16:46:00.559524Z","iopub.status.idle":"2024-04-27T16:46:00.567446Z","shell.execute_reply.started":"2024-04-27T16:46:00.55949Z","shell.execute_reply":"2024-04-27T16:46:00.566502Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\n\n# Save the model\ntorch.save(model.state_dict(), '/kaggle/working/speech_recognition_model.pt')\n","metadata":{"execution":{"iopub.status.busy":"2024-04-27T16:46:02.214507Z","iopub.execute_input":"2024-04-27T16:46:02.21537Z","iopub.status.idle":"2024-04-27T16:46:02.357126Z","shell.execute_reply.started":"2024-04-27T16:46:02.215336Z","shell.execute_reply":"2024-04-27T16:46:02.356324Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load the saved model\n# Define your model architecture\nmodel = SpeechRecognitionModel(pipeline_params['n_cnn_layers'], \n                               pipeline_params['n_rnn_layers'], \n                               pipeline_params['rnn_dim'], \n                               pipeline_params['n_class'], \n                               pipeline_params['n_feats'], \n                               pipeline_params['stride'], \n                               pipeline_params['dropout']).to(device)\n\n# Load the model\nmodel.load_state_dict(torch.load('/kaggle/working/speech_recognition_model.pt'))\n\n# Create instances of TextTransform and valid_audio_transforms\ntext_transform = TextTransform()\nvalid_audio_transforms = torchaudio.transforms.MelSpectrogram()\n\n# Example usage\naudio_path = '/kaggle/input/librispeech/LibriSpeech/dev-clean/174/50561/174-50561-0002.wav'\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\npredicted_text = speech_to_text(audio_path, model, device, text_transform, valid_audio_transforms)\nprint(f\"Predicted Text: {predicted_text}\")","metadata":{"execution":{"iopub.status.busy":"2024-04-27T16:46:17.861947Z","iopub.execute_input":"2024-04-27T16:46:17.862348Z","iopub.status.idle":"2024-04-27T16:46:18.362121Z","shell.execute_reply.started":"2024-04-27T16:46:17.862318Z","shell.execute_reply":"2024-04-27T16:46:18.361235Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null}]}